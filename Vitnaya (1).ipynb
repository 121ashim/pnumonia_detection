{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjipVTfXdH_m"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "33kxEguTdb2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #====================== Paths ======================\n",
        "base_path = \"/content/drive/MyDrive/pneumonia detectio sytem/dataset/chest_xray\"\n",
        "train_dir = Path(os.path.join(base_path, \"train\"))\n",
        "val_dir = Path(os.path.join(base_path, \"val\"))\n",
        "test_dir = Path(os.path.join(base_path, \"test\"))"
      ],
      "metadata": {
        "id": "TJgvyAKCdp7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================== Display Sample Images ======================\n",
        "def display_images(folder, num=3):\n",
        "    classes = ['NORMAL', 'PNEUMONIA', 'NOT_CHEST']\n",
        "    extensions = ['*.jpeg', '*.jpg', '*.png', '*.JPEG', '*.JPG', '*.PNG']\n",
        "    fig, axes = plt.subplots(nrows=len(classes), ncols=num, figsize=(15, 5))\n",
        "    for row, cls in enumerate(classes):\n",
        "        class_path = folder / cls\n",
        "        images = []\n",
        "        for ext in extensions:\n",
        "            images.extend(class_path.glob(ext))\n",
        "        images = images[:num]\n",
        "        for i in range(num):\n",
        "            if i < len(images):\n",
        "                img = Image.open(images[i])\n",
        "                axes[row, i].imshow(img, cmap='gray')\n",
        "                axes[row, i].set_title(cls)\n",
        "                axes[row, i].axis('off')\n",
        "            else:\n",
        "                axes[row, i].axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "display_images(train_dir)\n"
      ],
      "metadata": {
        "id": "kMA8k5MHdvTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================== Transforms ======================\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "val_test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "luajlTK5d2l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================== Load Data ======================\n",
        "train_dataset = ImageFolder(train_dir, transform=train_transforms)\n",
        "val_dataset = ImageFolder(val_dir, transform=val_test_transforms)\n",
        "test_dataset = ImageFolder(test_dir, transform=val_test_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "GtFyoAuad7yH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================== Compute Class Weights ======================\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(train_dataset.targets),\n",
        "    y=train_dataset.targets\n",
        ")\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float)"
      ],
      "metadata": {
        "id": "6ow-90G6d_pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================== Load ViT Model ======================\n",
        "config = ViTConfig.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "config.num_labels = 3\n",
        "\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    \"google/vit-base-patch16-224-in21k\", config=config\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "class_weights = class_weights.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
      ],
      "metadata": {
        "id": "9GcqgP2DeRkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.amp import autocast  # For mixed precision training\n",
        "from torch.cuda.amp import GradScaler  # Scales gradients to avoid underflow during AMP\n",
        "\n",
        "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=20):\n",
        "    # Lists to store losses and accuracies for each epoch\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accuracies, val_accuracies = [], []\n",
        "\n",
        "\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\n📘 Epoch {epoch+1}/{num_epochs}\")\n",
        "        model.train()  # Set model to training mode\n",
        "        running_loss, correct, total = 0.0, 0, 0  # Track training loss and accuracy\n",
        "\n",
        "        # ---------- TRAINING LOOP ----------\n",
        "        for images, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\", leave=False):\n",
        "            images, labels = images.to(device), labels.to(device)  # Move data to GPU/CPU\n",
        "            optimizer.zero_grad()  # Clear previous gradients\n",
        "\n",
        "            # Forward pass with automatic mixed precision\n",
        "            with autocast(device_type=device.type):\n",
        "                outputs = model(images).logits  # Forward pass (ViT model returns .logits)\n",
        "                loss = criterion(outputs, labels)  # Compute loss\n",
        "\n",
        "            # Backpropagation using scaled loss\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)  # Accumulate loss\n",
        "            _, preds = outputs.max(1)  # Get class predictions\n",
        "            correct += (preds == labels).sum().item()  # Count correct predictions\n",
        "            total += labels.size(0)  # Count total samples\n",
        "\n",
        "        # Compute average training loss and accuracy\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_acc = 100 * correct / total\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "\n",
        "        # ---------- VALIDATION LOOP ----------\n",
        "        model.eval()  # Set model to evaluation mode\n",
        "        val_loss, correct, total = 0.0, 0, 0\n",
        "        all_preds, all_labels = [], []  # Store predictions and labels for metrics\n",
        "\n",
        "        with torch.no_grad():  # Disable gradient calculation\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(images).logits  # Forward pass\n",
        "                loss = criterion(outputs, labels)  # Compute loss\n",
        "                val_loss += loss.item() * images.size(0)\n",
        "\n",
        "                _, preds = outputs.max(1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "                all_preds.extend(preds.cpu().numpy())  # Save predictions\n",
        "                all_labels.extend(labels.cpu().numpy())  # Save true labels\n",
        "\n",
        "        # Compute average validation loss and accuracy\n",
        "        val_loss = val_loss / len(val_loader.dataset)\n",
        "        val_acc = 100 * correct / total\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        # Compute additional metrics (for multi-class classification)\n",
        "        val_precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "        val_recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "        val_f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "\n",
        "        # Print results for this epoch\n",
        "        print(f\"✅ Epoch {epoch+1} | Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}% | \"\n",
        "              f\"Precision: {val_precision:.2f}, Recall: {val_recall:.2f}, F1: {val_f1:.2f}\")\n",
        "\n",
        "\n",
        "    # Return the model and all metrics for further analysis or plotting\n",
        "    return model, train_losses, val_losses, train_accuracies, val_accuracies\n"
      ],
      "metadata": {
        "id": "UO2RHMcEedWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================== Train Model ======================\n",
        "trained_model, train_losses, val_losses, train_accuracies, val_accuracies = train_model(\n",
        "    model, criterion, optimizer, train_loader, val_loader, num_epochs=20\n",
        ")"
      ],
      "metadata": {
        "id": "7qlW6U18ekpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================== Plot Accuracy & Loss ======================\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_accuracies, label='Train Accuracy')\n",
        "plt.plot(val_accuracies, label='Val Accuracy', color='orange')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Val Loss', color='orange')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Q3E_Y625es_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================== Evaluate with Confusion Matrix ======================\n",
        "def evaluate_model_with_confusion(model, test_loader, class_names):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images).logits\n",
        "            _, preds = outputs.max(1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "    acc = 100 * sum(p == t for p, t in zip(y_pred, y_true)) / len(y_true)\n",
        "    print(f\"\\n📊 Test Accuracy: {acc:.2f}%\")\n",
        "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "    print(f\"🔍 Precision: {precision:.4f}\")\n",
        "    print(f\"🔍 Recall:    {recall:.4f}\")\n",
        "    print(f\"🔍 F1 Score:  {f1:.4f}\")\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis],\n",
        "                annot=True, fmt='.2f', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.title(\"Normalized Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "evaluate_model_with_confusion(trained_model, test_loader, class_names=train_dataset.classes)"
      ],
      "metadata": {
        "id": "omrC6XD9et2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dhwIXFbWe0jm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}